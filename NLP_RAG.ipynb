{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG approach on an open source LLM such as Llama or PALM or Gemini with a vector db using langchain, DSPy etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Tasks and Comments**                                               | **Status** | **Individual Responsible** |\n",
    "|---------------------------------------------------------------------|------------|----------------------------|\n",
    "| **Using RAG approach on an open source LLM such as Llama or PALM or Gemini with a vector db using langchain, DSPy etc.** |            |                            |\n",
    "| **Preprocessing Steps** - 1. Clean up unwanted characters, 2. Extract Pairs, 3. Handling Abbreviations 4. Replace Slang 5. Negation Handling 6. Stopword removal 6. NER                   | Done | Kamalpreet Kaur             |\n",
    "| Training - Model built            | Done | Abhijeet Singh              |\n",
    "| **Evaluation - ROUGE-L Score (0.7647518193361473), BERT Score(0.9388486438989639)**                          | Done| Abhijeet Singh               |\n",
    "| Interpretation using Lime                                           | Not Applicable |                            |\n",
    "| 1st round of tuning - What was the issue faced/tuned? Ans:- **Not much contextually Aware** (Solution:- Decreased Temperature parameter)              | Done |    Abhijeet Singh                        |\n",
    "| 2nd round of tuning - What was the issue faced/tuned? Ans:- **Answer was not much precise**(Added parameter Num_beam and Num_return_Sequence )             | Done |    Abhijeet Singh                        |\n",
    "| Final AUC value?                              | Not Applicable |                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/singh/Downloads/NLP-1/chat_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       conversations          id\n",
      "0  [{'from': 'human', 'value': \"I've been feeling...  identity_0\n",
      "1  [{'from': 'human', 'value': \"Hi, I'm feeling r...  identity_1\n",
      "2  [{'from': 'human', 'value': \"Hey, I hope you'r...  identity_2\n",
      "3  [{'from': 'human', 'value': \"I'm feeling reall...  identity_3\n",
      "4  [{'from': 'human', 'value': \"I'm feeling reall...  identity_4\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean up the unwanted characters\n",
    "def clean_conversation(conversation):\n",
    "    # Replace single quotes with double quotes (standardizing quote format)\n",
    "    conversation = conversation.replace(\"'\", '\"')\n",
    "    \n",
    "    # Remove unnecessary escape characters like \\\"\n",
    "    conversation = re.sub(r'\\\\([\"\\'])', r'\\1', conversation)\n",
    "    \n",
    "    # Remove any stray backslashes at the end of the conversation\n",
    "    conversation = re.sub(r'\\\\$', '', conversation)\n",
    "    \n",
    "    # Ensure that the conversation is properly enclosed in double quotes\n",
    "    if not conversation.startswith('\"'):\n",
    "        conversation = '\"' + conversation\n",
    "    if not conversation.endswith('\"'):\n",
    "        conversation = conversation + '\"'\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "# Function to extract human-GPT pairs\n",
    "def extract_pairs(conversation):\n",
    "    # Split the conversation by 'from' and 'value' to get human and GPT responses\n",
    "    pairs = []\n",
    "    conversation_data = re.findall(r'{\"from\": \"(human|gpt)\", \"value\": \"(.*?)\"}', conversation)\n",
    "    \n",
    "    # Group the pairs\n",
    "    human_msg = None\n",
    "    for speaker, message in conversation_data:\n",
    "        if speaker == 'human':\n",
    "            human_msg = message\n",
    "        elif speaker == 'gpt' and human_msg:\n",
    "            pairs.append({'human': human_msg, 'gpt': message})\n",
    "            human_msg = None  # Reset for the next pair\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Clean the conversations column\n",
    "df['cleaned_conversations'] = df['conversations'].apply(clean_conversation)\n",
    "\n",
    "# Extract human-GPT pairs\n",
    "flattened_conversations = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    pairs = extract_pairs(row['cleaned_conversations'])\n",
    "    if pairs:\n",
    "        for pair in pairs:\n",
    "            flattened_conversations.append({\n",
    "                'id': row['id'],\n",
    "                'human': pair['human'],\n",
    "                'gpt': pair['gpt']\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the valid pairs\n",
    "flattened_df = pd.DataFrame(flattened_conversations)\n",
    "\n",
    "# Save the result to a new CSV\n",
    "flattened_df.to_csv('C:/Users/singh/Downloads/NLP-1/preprocessed_conversations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_df = pd.read_csv('C:/Users/singh/Downloads/NLP-1/preprocessed_conversations.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              human  \\\n",
      "0  identity_0  I\"ve been feeling so sad and overwhelmed latel...   \n",
      "1  identity_0  I recently got a promotion at work, which I th...   \n",
      "2  identity_0  Well, the workload has increased significantly...   \n",
      "3  identity_0  I\"ve been trying to prioritize my tasks and de...   \n",
      "4  identity_0  You\"re right. I haven\"t really opened up about...   \n",
      "\n",
      "                                                 gpt  \n",
      "0  Hey there, I\"m here to listen and support you....  \n",
      "1  I can understand how it can be overwhelming wh...  \n",
      "2  It sounds like you\"re dealing with a lot of pr...  \n",
      "3  It\"s great to hear that you\"re already impleme...  \n",
      "4  It\"s completely normal to feel that way, but r...  \n"
     ]
    }
   ],
   "source": [
    "print(New_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1. Define the function to expand abbreviations\n",
    "def expand_abbreviations(text):\n",
    "    # Replace all double quotes with apostrophes before applying the abbreviation expansion\n",
    "    text = text.replace('\"', \"'\")\n",
    "    \n",
    "    abbreviations = {\n",
    "        \"I'm\": \"I am\", \"you're\": \"you are\",\"It's\": \"It is\", \"it's\": \"it is\", \"can't\": \"cannot\",\n",
    "        \"don't\": \"do not\", \"I've\": \"I have\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "        \"they're\": \"they are\", \"we're\": \"we are\", \"isn't\": \"is not\", \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"won't\": \"will not\",\n",
    "        \"didn't\": \"did not\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"wouldn't\": \"would not\",\n",
    "        \"there's\": \"there is\",\"There's\": \"There is\", \"That's\": \"That is\",\"that's\": \"that is\", \"What's\": \"What is\", \"what's\": \"what is\", \"let's\": \"let us\", \"Let's\": \"Let us\",\n",
    "        \"who's\": \"who is\",\"Who's\": \"Who is\", \"aren't\": \"are not\"\n",
    "    }\n",
    "    for key, value in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(key) + r'\\b', value, text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to both human and GPT responses\n",
    "New_df['human'] = New_df['human'].apply(expand_abbreviations)\n",
    "New_df['gpt'] = New_df['gpt'].apply(expand_abbreviations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              human  \\\n",
      "0  identity_0  I\"ve been feeling so sad and overwhelmed latel...   \n",
      "1  identity_0  I recently got a promotion at work, which I th...   \n",
      "2  identity_0  Well, the workload has increased significantly...   \n",
      "3  identity_0  I\"ve been trying to prioritize my tasks and de...   \n",
      "4  identity_0  You\"re right. I haven\"t really opened up about...   \n",
      "\n",
      "                                                 gpt  \n",
      "0  Hey there, I\"m here to listen and support you....  \n",
      "1  I can understand how it can be overwhelming wh...  \n",
      "2  It sounds like you\"re dealing with a lot of pr...  \n",
      "3  It\"s great to hear that you\"re already impleme...  \n",
      "4  It\"s completely normal to feel that way, but r...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the output\n",
    "print(New_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slang(text):\n",
    "    slang_map = {\n",
    "        \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\",\n",
    "        \"ain't\": \"is not\", \"gimme\": \"give me\", \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\", \"lemme\": \"let me\", \"outta\": \"out of\",\n",
    "        \"dunno\": \"do not know\", \"bro\": \"brother\", \"sis\": \"sister\",\n",
    "        \"idk\": \"I do not know\", \"omg\": \"oh my god\", \"btw\": \"by the way\"\n",
    "    }\n",
    "    words = text.split()\n",
    "    processed_words = [slang_map.get(word.lower(), word.lower()) for word in words]\n",
    "    return ' '.join(processed_words)\n",
    "# Apply replace_slang to both 'human' and 'gpt' columns\n",
    "New_df['human'] = New_df['human'].apply(replace_slang)\n",
    "New_df['gpt'] = New_df['gpt'].apply(replace_slang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              human  \\\n",
      "0  identity_0  I\"ve been feeling so sad and overwhelmed latel...   \n",
      "1  identity_0  I recently got a promotion at work, which I th...   \n",
      "2  identity_0  Well, the workload has increased significantly...   \n",
      "3  identity_0  I\"ve been trying to prioritize my tasks and de...   \n",
      "4  identity_0  You\"re right. I haven\"t really opened up about...   \n",
      "\n",
      "                                                 gpt  \n",
      "0  Hey there, I\"m here to listen and support you....  \n",
      "1  I can understand how it can be overwhelming wh...  \n",
      "2  It sounds like you\"re dealing with a lot of pr...  \n",
      "3  It\"s great to hear that you\"re already impleme...  \n",
      "4  It\"s completely normal to feel that way, but r...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the output\n",
    "print(New_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negations(text):\n",
    "    negation_words = [\"not\", \"no\", \"never\", \"cannot\", \"n't\"]\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    negate = False\n",
    "\n",
    "    for word in words:\n",
    "        if any(neg in word.lower() for neg in negation_words):\n",
    "            if not negate:  # Activate negation only if it's not already active\n",
    "                processed_words.append(word.lower())\n",
    "            negate = True\n",
    "        elif negate:\n",
    "            # Apply negation to the next word and reset\n",
    "            processed_words.append(f\"not {word.lower()}\")\n",
    "            negate = False\n",
    "        else:\n",
    "            processed_words.append(word.lower())\n",
    "    \n",
    "    # Remove consecutive \"not not\" cases\n",
    "    final_text = ' '.join(processed_words).replace(\"not not\", \"not\")\n",
    "    return final_text\n",
    "\n",
    "# Apply the function to your dataframe\n",
    "New_df['human'] = New_df['human'].apply(handle_negations)\n",
    "New_df['gpt'] = New_df['gpt'].apply(handle_negations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              human  \\\n",
      "0  identity_0  I\"ve been feeling so sad and overwhelmed latel...   \n",
      "1  identity_0  I recently got a promotion at work, which I th...   \n",
      "2  identity_0  Well, the workload has increased significantly...   \n",
      "3  identity_0  I\"ve been trying to prioritize my tasks and de...   \n",
      "4  identity_0  You\"re right. I haven\"t really opened up about...   \n",
      "\n",
      "                                                 gpt  \n",
      "0  Hey there, I\"m here to listen and support you....  \n",
      "1  I can understand how it can be overwhelming wh...  \n",
      "2  It sounds like you\"re dealing with a lot of pr...  \n",
      "3  It\"s great to hear that you\"re already impleme...  \n",
      "4  It\"s completely normal to feel that way, but r...  \n"
     ]
    }
   ],
   "source": [
    "# Check for any lingering issues\n",
    "print(New_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for thorough text cleaning\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters (e.g., quotes, extra spaces)\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"-]', '', text)  # remove special characters\n",
    "    text = text.replace('\"', '').replace(\"'\", '')  # Remove quotes\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to both human and GPT responses\n",
    "New_df['human'] = New_df['human'].apply(clean_text)\n",
    "New_df['gpt'] = New_df['gpt'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              human  \\\n",
      "0  identity_0  I\"ve been feeling so sad and overwhelmed latel...   \n",
      "1  identity_0  I recently got a promotion at work, which I th...   \n",
      "2  identity_0  Well, the workload has increased significantly...   \n",
      "3  identity_0  I\"ve been trying to prioritize my tasks and de...   \n",
      "4  identity_0  You\"re right. I haven\"t really opened up about...   \n",
      "\n",
      "                                                 gpt  \n",
      "0  Hey there, I\"m here to listen and support you....  \n",
      "1  I can understand how it can be overwhelming wh...  \n",
      "2  It sounds like you\"re dealing with a lot of pr...  \n",
      "3  It\"s great to hear that you\"re already impleme...  \n",
      "4  It\"s completely normal to feel that way, but r...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the cleaned data\n",
    "print(New_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download stopwords and punkt if you haven't already\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of stopwords in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove negations from the stopwords list (to preserve them)\n",
    "negations = {\"not\", \"no\", \"nor\", \"never\", \"isn't\", \"aren't\", \"don't\", \"didn't\", \"won't\", \"can't\", \"shouldn't\"}\n",
    "stop_words -= negations  # Remove negations from the stopwords list\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(tokens):\n",
    "    # Remove punctuation from the token list\n",
    "    return [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Function to clean text (tokenize, remove stopwords and punctuation)\n",
    "def clean_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [word for word in remove_punctuation(tokens) if word.lower() not in stop_words]\n",
    "    # Join tokens back into a readable sentence\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning function to both 'human' and 'gpt' columns\n",
    "New_df['human_clean'] = New_df['human'].apply(clean_text)\n",
    "New_df['gpt_clean'] = New_df['gpt'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\Downloads\\NLP-1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained NER model from Hugging Face (using a model fine-tuned on NER tasks)\n",
    "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = 'C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(3000)\n",
    "\n",
    "# Drop rows with missing data in the columns 'human_clean' and 'gpt_clean'\n",
    "df = df.dropna(subset=['human_clean', 'gpt_clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mask named entities\n",
    "def mask_named_entities(text):\n",
    "    # Run NER to detect entities\n",
    "    entities = ner_model(text)\n",
    "    \n",
    "    # Sort entities by their 'start' position in reverse order to avoid overlap during replacement\n",
    "    entities = sorted(entities, key=lambda x: x['start'], reverse=True)\n",
    "    \n",
    "    # Replace entities with '<NAME>' in the text\n",
    "    for entity in entities:\n",
    "        # Check if the entity type is 'PER' (Person) or 'LOC' (Location)\n",
    "        if entity['entity'] in ['I-PER', 'I-LOC']:\n",
    "            text = text[:entity['start']] + '<NAME>' + text[entity['end']:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NAME> <NAME> visited the <NAME> <NAME> yesterday.\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Barack Obama visited the White House yesterday.\"\n",
    "print(mask_named_entities(test_text))  # Should return 'Barack Obama' (PER), 'White House' (ORG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# Apply NER-based name masking on the dataset columns\n",
    "df['human_clean'] = df['human_clean'].apply(mask_named_entities)\n",
    "df['gpt_clean'] = df['gpt_clean'].apply(mask_named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save the modified dataframe\n",
    "# df.to_csv('C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing_NER.csv', index=False)\n",
    "\n",
    "# # Check the first few rows of the modified dataframe\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing_NER.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path where the CSV will be saved\n",
    "save_path = 'C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing_NER.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(save_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"File saved successfully at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing Finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG approach on an open source LLM such as Llama or PALM or Gemini with a vector db using langchain, DSPy etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              human  \\\n",
      "0  identity_0  i have been feeling so sad and overwhelmed lat...   \n",
      "1  identity_0  i recently got a promotion at work, which i th...   \n",
      "2  identity_0  well, the workload has increased significantly...   \n",
      "3  identity_0  i have been trying to prioritize my tasks and ...   \n",
      "4  identity_0  youre right. i have not really opened up about...   \n",
      "\n",
      "                                                 gpt  \\\n",
      "0  hey there, i am here to listen and support you...   \n",
      "1  i can understand how it can be overwhelming wh...   \n",
      "2  it sounds like you are dealing with a lot of p...   \n",
      "3  it is great to hear that you are already imple...   \n",
      "4  it is completely normal not to feel that way, ...   \n",
      "\n",
      "                                         human_clean  \\\n",
      "0  feeling sad overwhelmed lately work become mas...   \n",
      "1  recently got promotion work thought would exci...   \n",
      "2  well workload increased significantly find har...   \n",
      "3  trying prioritize tasks delegate whenever not ...   \n",
      "4  youre right not really opened struggles cowork...   \n",
      "\n",
      "                                           gpt_clean  \n",
      "0  hey listen support sounds like work really cha...  \n",
      "1  understand overwhelming faced higher expectati...  \n",
      "2  sounds like dealing lot pressure perform succe...  \n",
      "3  great hear already implementing helpful strate...  \n",
      "4  completely normal not feel way remember asking...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "c:\\Users\\singh\\Downloads\\NLP-1\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG approach is a method of analyzing the relationship between a lm and\n",
      "Chatbot Response:\n",
      "Response: i would suggest you take a hot shower, take a hot bath,\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model to generate embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-distilroberta-v1\")\n",
    "\n",
    "# Load your data (adjust the file path as necessary)\n",
    "data_path = 'C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing_NER.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(100)\n",
    "\n",
    "# Preprocess and combine human and GPT text into one document\n",
    "df['combined'] = df['human_clean'] + \" \" + df['gpt_clean']\n",
    "\n",
    "# Generate embeddings for each combined text\n",
    "embeddings = np.array([embedding_model.encode(text) for text in df['combined']])\n",
    "\n",
    "\n",
    "\n",
    "# Create a FAISS index (Flat Index in this example)\n",
    "dim = embeddings.shape[1]  # Embedding dimension\n",
    "index = faiss.IndexFlatL2(dim)  # L2 distance metric for similarity search\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Create an IVF index\n",
    "nlist = 20  # Number of clusters (adjust based on dataset size)\n",
    "quantizer = faiss.IndexFlatL2(dim)  # Quantizer used for training\n",
    "index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "\n",
    "# Train the index\n",
    "index.train(embeddings)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# Function to retrieve the top-k most similar documents for a query\n",
    "def search_faiss(query, k=10):\n",
    "    # Convert query into an embedding\n",
    "    query_embedding = embedding_model.encode(query).reshape(1, -1)\n",
    "\n",
    "    # Perform similarity search\n",
    "    D, I = index.search(query_embedding, k)  # D is distances, I is indices of closest embeddings\n",
    "    \n",
    "    # Fetch the documents corresponding to the closest embeddings\n",
    "    results = [df.iloc[i] for i in I[0]]\n",
    "    \n",
    "    return results\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Model name\n",
    "llm_model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # Reduce memory usage   # Automatically map model to available GPU\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "\n",
    "# Define the pipeline \n",
    "llm = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "  \n",
    ")\n",
    "\n",
    "\n",
    "result = llm(\"Explain the concept of RAG approach on llms.\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "\n",
    "# Function to generate a chatbot response using the query and retrieved context\n",
    "def generate_response(query, k=5):\n",
    "    # Retrieve top-k similar documents\n",
    "    retrieved_results = search_faiss(query, k)\n",
    "    \n",
    "    # Combine results into a context string\n",
    "    context = \"\\n\".join([\n",
    "    f\"User said: {result['human_clean']}\\nResponse: {result['gpt_clean']}\"\n",
    "    for result in retrieved_results[:3]  # Use only the most relevant results\n",
    "    ])\n",
    "\n",
    "    # print(context)\n",
    "\n",
    "     \n",
    "    prompt = (\n",
    "    f\"Context:\\n{context}\\n\\n\"\n",
    "    f\"Query: {query}\\n\\n\"\n",
    "    f\"As an empathetic assistant, consider the user's situation and the context provided above. \"\n",
    "    f\"Respond with detailed and actionable advice that addresses their concerns thoughtfully.\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "    #Generate a response\n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example query\n",
    "query = \"what is best thing to do to deal with stress?\"\n",
    "response = generate_response(query)\n",
    "print(\"Chatbot Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "c:\\Users\\singh\\Downloads\\NLP-1\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG approach is a method of analyzing the relationship between a lm and its underlying variables.\n",
      "Chatbot Response:\n",
      "Response: i would suggest you take a hot shower, take a hot bath, or take a hot shower.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model to generate embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-distilroberta-v1\")\n",
    "\n",
    "# Load your data (adjust the file path as necessary)\n",
    "data_path = 'C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing_NER.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(100)\n",
    "\n",
    "# Preprocess and combine human and GPT text into one document\n",
    "df['combined'] = df['human_clean'] + \" \" + df['gpt_clean']\n",
    "\n",
    "# Generate embeddings for each combined text\n",
    "embeddings = np.array([embedding_model.encode(text) for text in df['combined']])\n",
    "\n",
    "\n",
    "\n",
    "# Create a FAISS index (Flat Index in this example)\n",
    "dim = embeddings.shape[1]  # Embedding dimension\n",
    "index = faiss.IndexFlatL2(dim)  # L2 distance metric for similarity search\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Create an IVF index\n",
    "nlist = 20  # Number of clusters (adjust based on dataset size)\n",
    "quantizer = faiss.IndexFlatL2(dim)  # Quantizer used for training\n",
    "index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "\n",
    "# Train the index\n",
    "index.train(embeddings)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# Function to retrieve the top-k most similar documents for a query\n",
    "def search_faiss(query, k=10):\n",
    "    # Convert query into an embedding\n",
    "    query_embedding = embedding_model.encode(query).reshape(1, -1)\n",
    "\n",
    "    # Perform similarity search\n",
    "    D, I = index.search(query_embedding, k)  # D is distances, I is indices of closest embeddings\n",
    "    \n",
    "    # Fetch the documents corresponding to the closest embeddings\n",
    "    results = [df.iloc[i] for i in I[0]]\n",
    "    \n",
    "    return results\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Model name\n",
    "llm_model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # Reduce memory usage   # Automatically map model to available GPU\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "\n",
    "# Define the pipeline \n",
    "llm = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.9 # Adjust for creativity\n",
    ")\n",
    "\n",
    "\n",
    "result = llm(\"Explain the concept of RAG approach on llms.\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "\n",
    "# Function to generate a chatbot response using the query and retrieved context\n",
    "def generate_response(query, k=5):\n",
    "    # Retrieve top-k similar documents\n",
    "    retrieved_results = search_faiss(query, k)\n",
    "    \n",
    "    # Combine results into a context string\n",
    "    context = \"\\n\".join([\n",
    "    f\"User said: {result['human_clean']}\\nResponse: {result['gpt_clean']}\"\n",
    "    for result in retrieved_results[:3]  # Use only the most relevant results\n",
    "    ])\n",
    "\n",
    "     \n",
    "    prompt = (\n",
    "    f\"Context:\\n{context}\\n\\n\"\n",
    "    f\"Query: {query}\\n\\n\"\n",
    "    f\"As an empathetic assistant, consider the user's situation and the context provided above. \"\n",
    "    f\"Respond with detailed and actionable advice that addresses their concerns thoughtfully.\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "    #Generate a response\n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example query\n",
    "query = \"what is best thing to do to deal with stress?\"\n",
    "response = generate_response(query)\n",
    "print(\"Chatbot Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run 3(Best)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unused GPU memory has been freed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "# Clear cache and force garbage collection\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Unused GPU memory has been freed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model to generate embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-distilroberta-v1\")\n",
    "\n",
    "# Load your data (adjust the file path as necessary)\n",
    "data_path = 'C:/Users/singh/Downloads/NLP-1/Possible_case_Preprocessing_NER.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(100)\n",
    "\n",
    "# Preprocess and combine human and GPT text into one document\n",
    "df['combined'] = df['human_clean'] + \" \" + df['gpt_clean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate embeddings for each combined text\n",
    "embeddings = np.array([embedding_model.encode(text) for text in df['combined']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a FAISS index (Flat Index in this example)\n",
    "dim = embeddings.shape[1]  # Embedding dimension\n",
    "index = faiss.IndexFlatL2(dim)  # L2 distance metric for similarity search\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Create an IVF index\n",
    "nlist = 20  # Number of clusters (adjust based on dataset size)\n",
    "quantizer = faiss.IndexFlatL2(dim)  # Quantizer used for training\n",
    "index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "\n",
    "# Train the index\n",
    "index.train(embeddings)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to retrieve the top-k most similar documents for a query\n",
    "def search_faiss(query, k=10):\n",
    "    # Convert query into an embedding\n",
    "    query_embedding = embedding_model.encode(query).reshape(1, -1)\n",
    "\n",
    "    # Perform similarity search\n",
    "    D, I = index.search(query_embedding, k)  # D is distances, I is indices of closest embeddings\n",
    "    \n",
    "    # Fetch the documents corresponding to the closest embeddings\n",
    "    results = [df.iloc[i] for i in I[0]]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: trying prioritize tasks delegate whenever not possible also started practicing meditation breaks help manage stress sometimes feels like no not matter not catch break constant struggle\n",
      "GPT: great hear already implementing helpful strategies remember progress takes time okay setbacks addition already encourage also communicate supervisor team workload discuss possible solutions together\n",
      "=\n",
      "Human: recently got promotion work thought would exciting added responsibilities pressure taken toll mental health really moving experience\n",
      "GPT: understand overwhelming faced higher expectations okay acknowledge not emotions allow feel sad situation important part healing process specific challenges facing work\n",
      "=\n",
      "Human: well recently go breakup thought moved not expect affect much additionally workload office increased adding stress\n",
      "GPT: breakups often lead wide range emotions normal not resurface unexpectedly seems like breakup coupled increased work stress might factors contributing current emotional state coping challenges far\n",
      "=\n",
      "Human: know not deep avoiding emotions not helping hard break free pattern\n",
      "GPT: change challenging especially comes processing emotions specific situations triggers intensify conflict within\n",
      "=\n",
      "Human: youre right not really opened struggles coworkers supervisor guess afraid appearing weak incapable approach discussion without feeling vulnerable\n",
      "GPT: completely normal not feel way remember asking support strength not weakness start scheduling conversation supervisor trusted colleague private comfortable setting honest challenges express willingness find solutions together remember not alone\n",
      "=\n",
      "Human: often feel like everyone else together one struggling tend think inherently flawed makes hard trust abilities\n",
      "GPT: remember everyone struggles insecurities even might not show important realize unique strengths talents make ever tried reframing negative thoughts\n",
      "=\n",
      "Human: constantly doubt abilities feel like never not good enough not hard confidence decisions afraid making mistakes\n",
      "GPT: sounds like self-esteem major issue recall specific experiences moments contributed lack trust\n",
      "=\n",
      "Human: think not good starting point thank providing guidance giving tools start making positive changes\n",
      "GPT: youre welcome remember change takes time patient ever need support guidance not hesitate reach\n",
      "=\n",
      "Human: think not good starting point thank providing guidance giving tools start making positive changes\n",
      "GPT: youre welcome remember change takes time patient ever need support guidance not hesitate reach\n",
      "=\n",
      "Human: think not good starting point thank providing guidance giving tools start making positive changes\n",
      "GPT: youre welcome remember change takes time patient ever need support guidance not hesitate reach\n",
      "=\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example query\n",
    "query = \"How can I manage stress at work?\"\n",
    "results = search_faiss(query)\n",
    "\n",
    "# Display the results (human_clean and gpt_clean columns)\n",
    "for result in results:\n",
    "    print(f\"Human: {result['human_clean']}\")\n",
    "    print(f\"GPT: {result['gpt_clean']}\")\n",
    "    print(\"=\"*1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Model name\n",
    "llm_model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # Reduce memory usage   # Automatically map model to available GPU\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the pipeline \n",
    "llm = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.4 # Adjust for creativity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\Downloads\\NLP-1\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG approach is a method of evaluating the effectiveness of a llm.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = llm(\"Explain the concept of RAG approach on llms.\")\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to generate a chatbot response using the query and retrieved context\n",
    "def generate_response(query, k=5):\n",
    "    # Retrieve top-k similar documents\n",
    "    retrieved_results = search_faiss(query, k)\n",
    "    \n",
    "    # Combine results into a context string\n",
    "    context = \"\\n\".join([\n",
    "    f\"User said: {result['human_clean']}\\nResponse: {result['gpt_clean']}\"\n",
    "    for result in retrieved_results[:3]  # Use only the most relevant results\n",
    "    ])\n",
    "\n",
    "    # print(context)\n",
    "\n",
    "     \n",
    "    prompt = (\n",
    "    f\"Context:\\n{context}\\n\\n\"\n",
    "    f\"Query: {query}\\n\\n\"\n",
    "    f\"As an empathetic assistant, consider the user's situation and the context provided above. \"\n",
    "    f\"Respond with detailed and actionable advice that addresses their concerns thoughtfully.\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "    #Generate a response\n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response:\n",
      "Response: I would recommend a stress reduction program that includes meditation, yoga, and relaxation techniques.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example query\n",
    "query = \"what is best thing to do to deal with stress?\"\n",
    "response = generate_response(query)\n",
    "print(\"Chatbot Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-L score: 0.7647518193361473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7647518193361473"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(df, k=5):\n",
    "    # Instantiate the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # List to store ROUGE scores\n",
    "    rouge_scores = []\n",
    "    \n",
    "    # Generate responses and compute ROUGE-L score\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row['human_clean']  # Use the human_clean column as the query\n",
    "        true_response = row['gpt_clean']  # Use the gpt_clean column as the true response\n",
    "        \n",
    "        # Generate response from the model\n",
    "        generated_response = generate_response(query, k)\n",
    "        \n",
    "        # Compute ROUGE-L score\n",
    "        score = scorer.score(true_response, generated_response)\n",
    "        rouge_scores.append(score['rougeL'].fmeasure)  # Append the ROUGE-L F-measure\n",
    "        \n",
    "    # Compute average ROUGE-L score\n",
    "    avg_rougeL = np.mean(rouge_scores)\n",
    "    print(f\"Average ROUGE-L score: {avg_rougeL}\")\n",
    "    return avg_rougeL\n",
    "\n",
    "# Evaluate the model on the dataset\n",
    "evaluate_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\Downloads\\NLP-1\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e987a0fd7a4c7f9b21e22f68d7fea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\Downloads\\NLP-1\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\singh\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51d4fb76519419595910ee77fba7ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ff84d5e96c499da34d2cae1db0149e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355e6563b6fd4eb2bbafc357d127f58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a35d8b53264011a1dc69be48634e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c47ee63eeba4a85977b3f5f5da89f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERT score: 0.9388486438989639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9388486438989639"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bert_score import score\n",
    "\n",
    "def evaluate_bert_score(df, k=5, limit=None):\n",
    "    # Limit the number of rows (if limit is specified)\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "    \n",
    "    # List to store BERT scores\n",
    "    bert_scores = []\n",
    "    \n",
    "    # Generate responses and compute BERT score\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row['human_clean']  # Use the human_clean column as the query\n",
    "        true_response = row['gpt_clean']  # Use the gpt_clean column as the true response\n",
    "        \n",
    "        # Generate response from the model\n",
    "        generated_response = generate_response(query, k)\n",
    "        \n",
    "        # Compute BERT score\n",
    "        P, R, F1 = score([generated_response], [true_response], lang='en')\n",
    "        bert_scores.append(F1.item())  # Append F1 score (which is the average of precision and recall)\n",
    "    \n",
    "    # Compute average BERT score\n",
    "    avg_bert_score = np.mean(bert_scores)\n",
    "    print(f\"Average BERT score: {avg_bert_score}\")\n",
    "    return avg_bert_score\n",
    "\n",
    "# Evaluate the model on the dataset with a limit of 100 samples for faster results\n",
    "evaluate_bert_score(df, limit=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
