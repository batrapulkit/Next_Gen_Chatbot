{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"chat_data.csv\")  # Ensure your dataset is in the same directory or provide the correct path\n",
    "\n",
    "# Columns: id, human, gpt, cleaned_human, cleaned_gpt\n",
    "# df = df[[\"cleaned_human\", \"cleaned_gpt\"]].dropna()\n",
    "\n",
    "df = df.sample(n=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset for Training and Evaluation\n",
    "df_train = df.sample(frac=0.8, random_state=42)  # 80% for training\n",
    "df_eval = df.drop(df_train.index)  # 20% for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "eval_dataset = Dataset.from_pandas(df_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer and Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da7298c44a148ed9c00156dbf41e319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2278210c9cba4df6820db6d084051bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the Dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"human\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(examples[\"gpt\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply the tokenize function to the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"human\", \"gpt\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"human\", \"gpt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pulkit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Round 1 Fine-Tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df523f49a6f8427198c55903de299b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6804, 'grad_norm': 1.8200733661651611, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5987, 'grad_norm': 3.172576427459717, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3c56284c4c4f8ba50535ac0909d3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5896318554878235, 'eval_runtime': 130.8912, 'eval_samples_per_second': 7.64, 'eval_steps_per_second': 1.91, 'epoch': 1.0}\n",
      "{'loss': 0.5786, 'grad_norm': 3.4807076454162598, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.574, 'grad_norm': 1.1021109819412231, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee58c39774664c9cb427cb8ed5d713af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5779553651809692, 'eval_runtime': 133.5398, 'eval_samples_per_second': 7.488, 'eval_steps_per_second': 1.872, 'epoch': 2.0}\n",
      "{'loss': 0.5578, 'grad_norm': 1.1667473316192627, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
      "{'loss': 0.561, 'grad_norm': 2.0330874919891357, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed01e1760cd4e5bbba0caae120c6af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5777411460876465, 'eval_runtime': 138.5322, 'eval_samples_per_second': 7.219, 'eval_steps_per_second': 1.805, 'epoch': 3.0}\n",
      "{'train_runtime': 9628.3197, 'train_samples_per_second': 1.246, 'train_steps_per_second': 0.312, 'train_loss': 0.5917533467610677, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.5917533467610677, metrics={'train_runtime': 9628.3197, 'train_samples_per_second': 1.246, 'train_steps_per_second': 0.312, 'total_flos': 3135504384000000.0, 'train_loss': 0.5917533467610677, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Fine-Tuning Round\n",
    "training_args_round1 = TrainingArguments(\n",
    "    output_dir=\"./results_round1\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_round1\"\n",
    ")\n",
    "\n",
    "trainer_round1 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_round1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Starting Round 1 Fine-Tuning...\")\n",
    "trainer_round1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2_round1\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2_round1\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2_round1\\\\vocab.json',\n",
       " 'fine_tuned_gpt2_round1\\\\merges.txt',\n",
       " 'fine_tuned_gpt2_round1\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model after the first round of fine-tuning\n",
    "model.save_pretrained(\"fine_tuned_gpt2_round1\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_round1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model for the second round\n",
    "model_round2 = GPT2LMHeadModel.from_pretrained(\"fine_tuned_gpt2_round1\")\n",
    "\n",
    "# Second Fine-Tuning Round\n",
    "training_args_round2 = TrainingArguments(\n",
    "    output_dir=\"./results_round2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_round2\"\n",
    ")\n",
    "\n",
    "trainer_round2 = Trainer(\n",
    "    model=model_round2,\n",
    "    args=training_args_round2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Starting Round 2 Fine-Tuning...\")\n",
    "trainer_round2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after the second round of fine-tuning\n",
    "model_round2.save_pretrained(\"fine_tuned_gpt2_round2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_round2\")\n",
    "\n",
    "print(\"Two Rounds of Fine-Tuning Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load Metrics\n",
    "rouge = evaluate.load(\"rouge\")  # Use `evaluate.load` instead of `load_metric`\n",
    "bert_scorer = evaluate.load(\"bertscore\")  # Use `evaluate.load` instead of `load_metric`\n",
    "\n",
    "# Example function to calculate ROUGE-L and BERTScore\n",
    "def evaluate_responses(eval_dataset, model, tokenizer):\n",
    "    inputs = eval_dataset[\"cleaned_human\"]\n",
    "    references = eval_dataset[\"cleaned_gpt\"]\n",
    "    predictions = []\n",
    "\n",
    "    for inp in inputs:\n",
    "        input_ids = tokenizer(inp, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        output_ids = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=50256)\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # ROUGE-L\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    print(\"ROUGE-L Score:\", rouge_results[\"rougeL\"])\n",
    "\n",
    "    # BERTScore\n",
    "    bert_results = bert_scorer.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    print(\"BERTScore F1:\", bert_results[\"f1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_scorer = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Function to Evaluate Responses\n",
    "def evaluate_responses(eval_dataset, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate the responses generated by the model using ROUGE-L and BERTScore.\n",
    "\n",
    "    Args:\n",
    "        eval_dataset: The dataset containing \"human\" and \"gpt\" columns.\n",
    "        model: The pre-trained language model to evaluate.\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure eval_dataset is properly formatted\n",
    "    if not all(col in eval_dataset.column_names for col in [\"human\", \"gpt\"]):\n",
    "        raise ValueError(\"The dataset must have 'human' and 'gpt' columns.\")\n",
    "\n",
    "    # Prepare inputs and references\n",
    "    inputs = eval_dataset[\"human\"]  # List of human inputs\n",
    "    references = eval_dataset[\"gpt\"]  # List of reference GPT outputs\n",
    "    predictions = []\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    for inp in inputs:\n",
    "        input_ids = tokenizer(inp, return_tensors=\"pt\", truncation=True, padding=True).input_ids.to(model.device)\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Evaluate ROUGE-L\n",
    "    print(\"Computing ROUGE-L...\")\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "    print(\"ROUGE-L Score:\", rouge_results[\"rougeL\"])\n",
    "\n",
    "    # Evaluate BERTScore\n",
    "    print(\"Computing BERTScore...\")\n",
    "    bert_results = bert_scorer.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    print(\"BERTScore Precision:\", bert_results[\"precision\"])\n",
    "    print(\"BERTScore Recall:\", bert_results[\"recall\"])\n",
    "    print(\"BERTScore F1:\", bert_results[\"f1\"])\n",
    "\n",
    "# Example usage:\n",
    "# evaluate_responses(eval_dataset, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import evaluate\n",
    "# # Metrics Calculation (ROUGE-L and BERTScore)\n",
    "# rouge = load_metric(\"rouge\")\n",
    "# bert_scorer = load_metric(\"bertscore\")\n",
    "\n",
    "# # Evaluate Responses\n",
    "# def evaluate_responses(eval_dataset, model, tokenizer):\n",
    "#     inputs = eval_dataset[\"cleaned_human\"]\n",
    "#     references = eval_dataset[\"cleaned_gpt\"]\n",
    "#     predictions = []\n",
    "    \n",
    "#     for inp in inputs:\n",
    "#         input_ids = tokenizer(inp, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "#         output_ids = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=50256)\n",
    "#         pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "#         predictions.append(pred)\n",
    "    \n",
    "#     # ROUGE-L\n",
    "#     rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "#     print(\"ROUGE-L Score:\", rouge_results[\"rougeL\"])\n",
    "    \n",
    "#     # BERTScore\n",
    "#     bert_results = bert_scorer.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "#     print(\"BERTScore F1:\", bert_results[\"f1\"])\n",
    "\n",
    "# print(\"Evaluating Chatbot...\")\n",
    "# evaluate_responses(df_eval, model_round2, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lime.lime_text import LimeTextExplainer\n",
    "# import torch\n",
    "\n",
    "# # Initialize the explainer\n",
    "# explainer = LimeTextExplainer(class_names=[\"Response Quality\"])\n",
    "\n",
    "# def explain_prediction(text, model, tokenizer):\n",
    "#     # Explicitly move the model to CPU\n",
    "#     model = model.to(\"cpu\")\n",
    "    \n",
    "#     def predict_proba(inputs):\n",
    "#         # Tokenize inputs with attention mask and left padding\n",
    "#         encoding = tokenizer(inputs, return_tensors=\"pt\", padding='longest', truncation=True, return_attention_mask=True, padding_side='left')\n",
    "        \n",
    "#         input_ids = encoding.input_ids.to(\"cpu\")  # Move to CPU\n",
    "#         attention_mask = encoding.attention_mask.to(\"cpu\")  # Move attention mask to CPU\n",
    "        \n",
    "#         # Generate output tokens\n",
    "#         with torch.no_grad():  # Avoid gradient tracking\n",
    "#             generated_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "        \n",
    "#         # Convert generated_ids to probabilities (or you can directly return them)\n",
    "#         return generated_ids.detach().cpu().numpy()  # Return token IDs as output for LIME\n",
    "\n",
    "#     # Explain the instance\n",
    "#     explanation = explainer.explain_instance(text, predict_proba, num_features=10)\n",
    "#     explanation.show_in_notebook()\n",
    "\n",
    "# # Example usage\n",
    "# print(\"Explaining a response with LIME...\")\n",
    "# explain_prediction(\"Why is the sky blue?\", model_round2, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "# import torch\n",
    "\n",
    "# def calculate_auc(model, eval_dataset, tokenizer):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     true_labels = []\n",
    "#     probabilities = []\n",
    "\n",
    "#     # Example: Define token IDs for class 1 (positive class) and class 0 (negative class)\n",
    "#     class_1_token_ids = [100, 200, 300]  # Replace with actual token IDs for class 1\n",
    "#     class_0_token_ids = [400, 500, 600]  # Replace with actual token IDs for class 0\n",
    "\n",
    "#     for batch in eval_dataset:\n",
    "#         input_ids = torch.tensor(batch[\"input_ids\"]).to(model.device)  # Move tensor to device\n",
    "#         attention_mask = torch.tensor(batch[\"attention_mask\"]).to(model.device)  # Move tensor to device\n",
    "#         labels = batch[\"labels\"]  # Assuming labels are token IDs\n",
    "        \n",
    "#         # Inspecting the labels to ensure correct mapping\n",
    "#         print(\"Sample of labels in batch:\", labels[:20])  # Print first 20 labels to inspect\n",
    "\n",
    "#         # Map token IDs to binary labels: 1 for class_1_token_ids, 0 for others\n",
    "#         binary_labels = [1 if token_id in class_1_token_ids else 0 for token_id in labels]\n",
    "        \n",
    "#         # Check label distribution in the batch\n",
    "#         print(\"Label distribution in batch:\", torch.bincount(torch.tensor(binary_labels)))\n",
    "        \n",
    "#         with torch.no_grad():  # Disable gradient computation\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "#         logits = outputs.logits.detach().cpu().numpy()  # Extract logits on CPU\n",
    "#         prob = torch.sigmoid(torch.tensor(logits)).numpy()[:, 1]  # Probability for class 1\n",
    "#         probabilities.extend(prob)\n",
    "        \n",
    "#         true_labels.extend(binary_labels)  # Use the binary labels\n",
    "\n",
    "#     # Ensure the length matches\n",
    "#     assert len(true_labels) == len(probabilities), \"Mismatch in lengths of true labels and probabilities\"\n",
    "\n",
    "#     # Flatten labels and probabilities to 1D arrays\n",
    "#     true_labels = torch.tensor(true_labels).flatten()\n",
    "#     probabilities = torch.tensor(probabilities).flatten()\n",
    "\n",
    "#     # Check the distribution of labels\n",
    "#     print(\"Distribution of true labels:\", torch.bincount(true_labels))\n",
    "\n",
    "#     # If only one class is present, use average precision score instead\n",
    "#     if len(torch.unique(true_labels)) == 1:\n",
    "#         print(\"Only one class is present in true labels. Using Precision-Recall AUC instead.\")\n",
    "        \n",
    "#         # Inspect the true labels and predicted probabilities for debugging\n",
    "#         print(\"True labels:\", true_labels[:20])  # Print first 20 true labels\n",
    "#         print(\"Predicted probabilities:\", probabilities[:20])  # Print first 20 probabilities\n",
    "        \n",
    "#         auc_score = average_precision_score(true_labels, probabilities)\n",
    "#         print(\"AUC-PR Score:\", auc_score)\n",
    "#     else:\n",
    "#         # Calculate the AUC score\n",
    "#         auc_score = roc_auc_score(true_labels, probabilities)\n",
    "#         print(\"AUC Score:\", auc_score)\n",
    "\n",
    "# # Call the function to calculate AUC\n",
    "# calculate_auc(model_round2, eval_dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at D:/Downloads/chatts/fine_tuned_gpt2_round1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"D:/Downloads/chatts/fine_tuned_gpt2_round1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/Downloads/chatts/fine_tuned_gpt2_round1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE: {'rouge1': 0.5, 'rouge2': 0.0, 'rougeL': 0.5, 'rougeLsum': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: tensor([0.9965])\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "rouge_result = rouge.compute(predictions=[\"generated text\"], references=[\"expected text\"])\n",
    "print(\"ROUGE:\", rouge_result)\n",
    "\n",
    "# For BERTScore\n",
    "import bert_score\n",
    "\n",
    "P, R, F1 = bert_score.score([\"generated text\"], [\"expected text\"], lang=\"en\")\n",
    "print(\"BERTScore F1:\", F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pulkit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Pulkit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: hi , how are you doing?\n",
      "\n",
      "Karen: I'm doing fine. I've been working on my game for a while now.\n",
      " (laughs)\n",
      ". . .\n",
      ", how do you feel about the new game? Do you\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT2 language model (for text generation)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Please enter your query: \")\n",
    "\n",
    "# Tokenizing the input\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "\n",
    "# Generating a response (using model.generate for text generation)\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],  # Input tokens\n",
    "    max_length=50,  # Limit response length (can adjust as needed)\n",
    "    num_return_sequences=1,  # You can adjust how many responses to generate\n",
    "    no_repeat_ngram_size=2,  # Prevent repeating n-grams (optional)\n",
    "    top_p=0.95,  # Nucleus sampling (optional)\n",
    "    temperature=0.7,  # Controls randomness (optional)\n",
    ")\n",
    "\n",
    "# Decoding the generated tokens into text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the generated response\n",
    "print(\"Chatbot Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: You are a helpful and polite assistant. Answer the following question: i am not feeling good. I am feeling very good and feeling very strong. I am feeling strong and feeling very excited. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am feeling very strong and feeling very happy. I am\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the pad token to the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Please enter your query: \")\n",
    "\n",
    "# Add a prompt for better context\n",
    "prompt = f\"You are a helpful and polite assistant. Answer the following question: {user_input}\"\n",
    "\n",
    "# Tokenizing the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate response with improved settings\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],  # Fix attention mask warning\n",
    "    max_length=150,\n",
    "    do_sample=True,  # Enable sampling for diverse output\n",
    "    top_p=0.95,      # Nucleus sampling\n",
    "    temperature=0.7, # Adds randomness\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Use the defined pad token\n",
    ")\n",
    "\n",
    "# Decoding the generated tokens into text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Output the generated response\n",
    "print(\"Chatbot Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n",
      "Chatbot: I know what you mean.\n",
      "\n",
      " ______________________________________________________________________________ [12/6] - Last edited by Dazzle on Sep 18, 2016 at 12 :45 PM; Edited 2 times in total â€¢\n",
      "Chatbot: You're right! My life is full of amazing things to come from this game and it's wonderful that my character can't be found because she was born here...I donÂ´t have a clue where her parents came From or how long ago they were taken away after being kidnapped but when we finally got there our story will tell the whole world about us :) \"And now hetery-dandy\" This message has been deleted .\n",
      "Chatbot: It really doesnï¿½ t matter if its hot (unless your car doesnt run) ...it always does anyway with me !oftheweather!!!\n",
      "Chatbot: And you know what I mean? A lot of people in New York City who dont want their cars parked on fire all day. The city gets so crowded, everyone drives into traffic at night like crazy everytime someone comes out for work lol!! :D\n",
      "\n",
      "(lol). So many times ive seen bad guys get shot by cops during rush hour..when these kindof officers would just start shooting them up.....and then suddenly some guy could say anything possible without even realizing....which means EVERYTHING?!?? Isnt anyone going\n",
      "Chatbot: Well my husband says it's because he has a boyfriend and is worried about getting hurt or killed when his friends die...because they are not good enough to do something that will save him! :( haha .I am trying very hard to stay positive but i can't seem keep track -_-\n",
      "\n",
      "Â [img]http://i.imgur-6WZLwA.jpg[/image][/url](https:/ /u/TPPStreamerBot 2015\n",
      "Chatbot: You're looking over your shoulder right now ! Do ya see me here ? What did ye find there thats ok buddy , we had no idea if its gonna\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained conversational model (DialoGPT)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Chatbot function\n",
    "def chatbot_response(user_input):\n",
    "    global conversation_history\n",
    "    \n",
    "    # Add user input to conversation history\n",
    "    conversation_history.append(f\"User: {user_input}\")\n",
    "    \n",
    "    # Prepare the model input\n",
    "    input_text = \"\\n\".join(conversation_history[-5:])  # Keep last 5 exchanges\n",
    "    input_ids = tokenizer.encode(input_text + \"\\nBot:\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate response\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=250,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.7,           # Adjust randomness\n",
    "        top_p=0.9,                 # Nucleus sampling for variety\n",
    "        do_sample=True,            # Enable sampling\n",
    "        repetition_penalty=1.2     # Penalize repetitive responses\n",
    "    )\n",
    "    \n",
    "    # Decode the output and add to history\n",
    "    bot_response = tokenizer.decode(output_ids[0], skip_special_tokens=True).split(\"Bot:\")[-1].strip()\n",
    "    conversation_history.append(f\"Bot: {bot_response}\")\n",
    "    \n",
    "    # Limit conversation history\n",
    "    if len(conversation_history) > 20:\n",
    "        conversation_history = conversation_history[-20:]\n",
    "    \n",
    "    return bot_response\n",
    "\n",
    "# Main interaction loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = chatbot_response(user_input)\n",
    "        \n",
    "        print(f\"Chatbot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_round2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_round2\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(eval_dataset)\n\u001b[0;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m eval_results\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m eval_results\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer_round2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer_round2.predict(eval_dataset)\n",
    "predictions = eval_results.predictions\n",
    "labels = eval_results.label_ids\n",
    "\n",
    "# If logits need to be converted to probabilities\n",
    "if len(predictions.shape) > 1:  # Multi-class classification\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "else:  # Binary classification\n",
    "    probabilities = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "\n",
    "# Compute AUC (multi-class example)\n",
    "auc = roc_auc_score(labels, probabilities, multi_class=\"ovr\")\n",
    "\n",
    "# Compute ROC curve for a specific class (optional, for binary class or one-vs-rest setup)\n",
    "fpr, tpr, _ = roc_curve(labels.ravel(), probabilities.ravel())\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
